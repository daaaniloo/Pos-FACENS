{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grupo\n",
    "* Danilo F Duarte - 203085\n",
    "* Darci Belon - 203022\n",
    "* Eduardo Mourão - 203072\n",
    "* Fábio Henrique - 203021\n",
    "* Victor Ciccolani - 203094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "#import da base\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declarando funções que serão usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declarando o conteúdo da atividade em funções (KNN, decision tree e random forest)\n",
    "tipo = []\n",
    "dados = []\n",
    "acc =[]\n",
    "time_l = []\n",
    "\n",
    "\n",
    "def knn_bagging(Xtrain,ytrain,Xtest,ytest, k):\n",
    "    #dados sem reducao\n",
    "    start = time.time()\n",
    "    model = KNeighborsClassifier()\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors simples:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "    \n",
    "    tip1 = 'Acurácia de KNeighbors simples:'\n",
    "    a1 = scores.mean()\n",
    "    t1 = end - start\n",
    "    acc.append(a1)\n",
    "    time_l.append(t1)\n",
    "    tipo.append(tip1)\n",
    "    d1 = k\n",
    "    dados.append(d1)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, random_state = 42)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors Bagging (c/ 10 estimators):', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_2 = model.predict(Xtest)\n",
    "    \n",
    "    tip2 = 'Acurácia de KNeighbors Bagging (c/ 10 estimators):'\n",
    "    a2 = scores.mean()\n",
    "    t2 = end - start\n",
    "    acc.append(a2)\n",
    "    time_l.append(t2)\n",
    "    tipo.append(tip2)\n",
    "    d2 = k\n",
    "    dados.append(d2)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, n_estimators=100, random_state = 42)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors Bagging (c/ 100 estimators):', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_3 = model.predict(Xtest)\n",
    "    \n",
    "    tip3 ='Acurácia de KNeighbors Bagging (c/ 100 estimators):'\n",
    "    a3 = scores.mean()\n",
    "    t3 = end - start\n",
    "    acc.append(a3)\n",
    "    time_l.append(t3) \n",
    "    tipo.append(tip3)\n",
    "    d3 = k\n",
    "    dados.append(d3)\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Teste do McNemar')\n",
    "    testaMcNemar(ytest,y_pred_1,y_pred_2)\n",
    "    testaMcNemar(ytest,y_pred_1,y_pred_3)\n",
    "    testaMcNemar(ytest,y_pred_2,y_pred_3)\n",
    "    \n",
    "    \n",
    "    return y_pred_1,y_pred_2,y_pred_3\n",
    "#################################################################################################\n",
    "    \n",
    "def decision_tree(Xtrain,ytrain,Xtest,ytest, k):\n",
    "    start = time.time()\n",
    "    model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Decision Tree puro:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "    \n",
    "    tip4 = 'Acurácia de Decision Tree puro:'\n",
    "    a4 = scores.mean()\n",
    "    t4 = end - start\n",
    "    acc.append(a4)\n",
    "    time_l.append(t4) \n",
    "    tipo.append(tip4)\n",
    "    d4 = k\n",
    "    dados.append(d4)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Random Forest:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_2 = model.predict(Xtest)\n",
    "    \n",
    "    tip5 = 'Acurácia de Random Forest:'\n",
    "    a5 = scores.mean()\n",
    "    t5 = end - start\n",
    "    acc.append(a5)\n",
    "    time_l.append(t5) \n",
    "    tipo.append(tip5)\n",
    "    d5 = k\n",
    "    dados.append(d5)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = ExtraTreesClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Extreme Randomized Trees:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_3 = model.predict(Xtest)\n",
    "    \n",
    "    tip6 = 'Acurácia de Random Forest:'\n",
    "    a6 = scores.mean()\n",
    "    t6 = end - start\n",
    "    acc.append(a6)\n",
    "    time_l.append(t6) \n",
    "    tipo.append(tip6)\n",
    "    d6 = k\n",
    "    dados.append(d6)\n",
    "        \n",
    "    print('-------------------------------------')\n",
    "    print('Teste do McNemar')\n",
    "    testaMcNemar(ytest,y_pred_1,y_pred_2)\n",
    "    testaMcNemar(ytest,y_pred_1,y_pred_3)\n",
    "    testaMcNemar(ytest,y_pred_2,y_pred_3)\n",
    "    \n",
    "    return y_pred_1,y_pred_2,y_pred_3\n",
    "########################################################################################################\n",
    "\n",
    "def multiples_vote(Xtrain,ytrain,Xtest,ytest):\n",
    "    start = time.time()\n",
    "    #Obs aumentei a max_inter da regressão logistica para tirar os warnings (deixou bem lento, tem a ver com a escala dos dados)\n",
    "    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter = 10000, random_state=1)\n",
    "    clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "    clf3 = GaussianNB()\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb',clf3)], voting='hard')\n",
    "\n",
    "    for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "    print('-'*20)\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n",
    "\n",
    "    for clf, label in zip([clf1, clf2, eclf], ['Logistic Regression', 'Random Forest', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "def gradient_boost(Xtrain,ytrain,Xtest,ytest, k):\n",
    "    \n",
    "    start = time.time()\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.3, max_depth=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "\n",
    "    print('Acurácia de Gradient Boosting Tree:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "    \n",
    "    tip7 = 'Acurácia de Gradient Boosting Tree:'\n",
    "    a7 = scores.mean()\n",
    "    t7 = end - start\n",
    "    acc.append(a7)\n",
    "    time_l.append(t7) \n",
    "    tipo.append(tip7)\n",
    "    d7 = k\n",
    "    dados.append(d7)\n",
    "    \n",
    "\n",
    "    return y_pred_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer a parte do mc nemmar: A ideia é fazer 1 comparação com cada tipo (originais,redução e seleção):\n",
    "\n",
    "#essa primeira função cria a contigence table a partir do y_test,y_pred1 e y_pred2 (resultados que vc quer comparar)\n",
    "def build_contingence_table(Y, Y_pred_1, Y_pred_2):\n",
    "    y1_and_y2 = 0\n",
    "    y1_and_not_y2 = 0\n",
    "    y2_and_not_y1 = 0\n",
    "    not_y1_and_not_y2 = 0\n",
    "    for y, y1, y2 in zip(Y, Y_pred_1, Y_pred_2):\n",
    "        if y == y1 == y2:\n",
    "            y1_and_y2 += 1\n",
    "        elif y != y1 and y != y2:\n",
    "            not_y1_and_not_y2 += 1\n",
    "        elif y == y1 and y != y2:\n",
    "            y1_and_not_y2 += 1\n",
    "        elif y != y1 and y == y2:\n",
    "            y2_and_not_y1 += 1\n",
    "            \n",
    "    contingency_table = [[y1_and_y2, y1_and_not_y2], \n",
    "                         [y2_and_not_y1, not_y1_and_not_y2]]\n",
    "    \n",
    "    return contingency_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testaMcNemar(test, pred1, pred2):\n",
    "    contingence_table = build_contingence_table(test, pred1, pred2)\n",
    "\n",
    "    import pprint\n",
    "\n",
    "    pprint.pprint(contingence_table)\n",
    "\n",
    "    result = mcnemar(contingence_table, exact=True)\n",
    "\n",
    "\n",
    "    if result.pvalue >= 0.001:\n",
    "        print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "    else:\n",
    "        print('statistic=%.3f, p-value=%.3e' % (result.statistic, result.pvalue))\n",
    "\n",
    "    # interpretando o p-value\n",
    "    alpha = 0.05\n",
    "    if result.pvalue > alpha:\n",
    "        print('Mesma proporção de erros (falhou em rejeitar H0)')\n",
    "    else:\n",
    "        print('Proporções de erros diferentes (rejeitou H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo em treino e teste para cada tipo (original, redução e seleção)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número original de atributos: 64\n",
      "Número reduzido de atributos: 21\n",
      "Número de atributos de Seleçao: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 0 32 39] are constant.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 0 16 24 32 39 56] are constant.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# Aplicando Train test split nos dados originais\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Número original de atributos:', X.shape[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Aplicando Train test split nos dados com reduçao de atributos (PCA)\n",
    "\n",
    "#realizando a redução\n",
    "pca = PCA(n_components=0.90, whiten=True)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Número reduzido de atributos:', X_pca.shape[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Aplicando Train test split na seleção de atributos\n",
    "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "fvalue_selector = SelectKBest(f_classif, k=20)\n",
    "X_kbest = fvalue_selector.fit_transform(X_train_sel, y_train_sel)\n",
    "X_kbest2= fvalue_selector.fit_transform(X_test_sel, y_test_sel)\n",
    "print('Número de atributos de Seleçao:', X_kbest.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocando tudo dentro de um dicionário e rodando com um loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'originais':[X_train, y_train, X_test, y_test],'redução':[X_train_pca, y_train_pca, X_test_pca, y_test_pca],\n",
    "     'seleção':[X_kbest, y_train_sel, X_kbest2, y_test_sel]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originais\n",
      "redução\n",
      "seleção\n"
     ]
    }
   ],
   "source": [
    "#essa parte é só um teste\n",
    "# observação d[k][0] é X e d[k][1] é y\n",
    "for k in d.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados originais :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9767150760719225\n",
      "Tempo: 0.1627061367034912\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9709163208852004\n",
      "Tempo: 0.3684539794921875\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9717461964038726\n",
      "Tempo: 3.8232011795043945\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[580, 10], [1, 3]]\n",
      "statistic=1.000, p-value=0.012\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[582, 8], [0, 4]]\n",
      "statistic=0.000, p-value=0.008\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[578, 3], [4, 9]]\n",
      "statistic=3.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8204426002766253\n",
      "Tempo: 0.06041073799133301\n",
      "Acurácia de Random Forest: 0.9592842323651452\n",
      "Tempo: 0.6413118839263916\n",
      "Acurácia de Extreme Randomized Trees: 0.971753112033195\n",
      "Tempo: 0.4829719066619873\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[503, 3], [75, 13]]\n",
      "statistic=3.000, p-value=5.238e-19\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[503, 3], [79, 9]]\n",
      "statistic=3.000, p-value=3.804e-20\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[575, 3], [7, 9]]\n",
      "statistic=3.000, p-value=0.344\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 35.938252210617065\n",
      "Acurácia de Gradient Boosting Tree: 0.9576106500691562\n",
      "Tempo: 11.849338054656982\n",
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados redução :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9692392807745505\n",
      "Tempo: 0.10333704948425293\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9625933609958507\n",
      "Tempo: 0.2538120746612549\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9734094052558783\n",
      "Tempo: 2.291666030883789\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[569, 11], [6, 8]]\n",
      "statistic=6.000, p-value=0.332\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[577, 3], [5, 9]]\n",
      "statistic=3.000, p-value=0.727\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[570, 5], [12, 7]]\n",
      "statistic=5.000, p-value=0.143\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8462102351313969\n",
      "Tempo: 0.09127020835876465\n",
      "Acurácia de Random Forest: 0.9534405255878285\n",
      "Tempo: 0.9929261207580566\n",
      "Acurácia de Extreme Randomized Trees: 0.9733955739972338\n",
      "Tempo: 0.53755784034729\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[489, 5], [80, 20]]\n",
      "statistic=5.000, p-value=1.806e-18\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[492, 2], [90, 10]]\n",
      "statistic=2.000, p-value=1.728e-24\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[566, 3], [16, 9]]\n",
      "statistic=3.000, p-value=0.004\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 38.278175354003906\n",
      "Acurácia de Gradient Boosting Tree: 0.9443222683264176\n",
      "Tempo: 17.841483116149902\n",
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados seleção :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9468084370677732\n",
      "Tempo: 0.08433723449707031\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9193845089903181\n",
      "Tempo: 0.20941710472106934\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9384958506224066\n",
      "Tempo: 1.8325910568237305\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[80, 18], [71, 425]]\n",
      "statistic=18.000, p-value=1.297e-08\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[79, 19], [11, 485]]\n",
      "statistic=11.000, p-value=0.200\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[82, 69], [8, 435]]\n",
      "statistic=8.000, p-value=3.137e-13\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8179598893499309\n",
      "Tempo: 0.03400707244873047\n",
      "Acurácia de Random Forest: 0.9360235131396957\n",
      "Tempo: 0.5897207260131836\n",
      "Acurácia de Extreme Randomized Trees: 0.9459716459197788\n",
      "Tempo: 0.4726133346557617\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[52, 69], [99, 374]]\n",
      "statistic=69.000, p-value=0.025\n",
      "Proporções de erros diferentes (rejeitou H0)\n",
      "[[52, 69], [83, 390]]\n",
      "statistic=69.000, p-value=0.292\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[93, 58], [42, 401]]\n",
      "statistic=42.000, p-value=0.133\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 37.74335694313049\n",
      "Acurácia de Gradient Boosting Tree: 0.9368222683264177\n",
      "Tempo: 8.167805910110474\n"
     ]
    }
   ],
   "source": [
    "#o d2 é mais um dicionário para armazenar todos os y_pred de cada modelo (ainda tenho que organiza-lo melhor)\n",
    "\n",
    "#OBS: FALTA FAZER A FUNÇÃO DO MULTIPLE VOTES JOGAR O Y_PRED NA VARIÁVEL (AINDA NÃO CONSEGUI)\n",
    "\n",
    "d2={}\n",
    "for k in d.keys():\n",
    "    print('==========================================================================')\n",
    "    print('Tempos e acurácias dos modelos com os dados',k,':')\n",
    "    print('==========================================================================')\n",
    "    knn_bagging(d[k][0],d[k][1],d[k][2], d[k][3], k)  \n",
    "    print('_____________________')\n",
    "    decision_tree(d[k][0],d[k][1],d[k][2], d[k][3],k)\n",
    "    #print('_____________________')\n",
    "    multiples_vote(d[k][0],d[k][1],d[k][2], d[k][3])\n",
    "    #print('_____________________')\n",
    "    y_pred_g1 = gradient_boost(d[k][0],d[k][1],d[k][2], d[k][3],k)\n",
    "    #d2[k + '_boost'] = y_pred_g1\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise = pd.DataFrame({'Modelo' : tipo,\n",
    "                 'Acuracia' : acc,\n",
    "                 'tempo' : time_l,\n",
    "                 'tipo': dados })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Acuracia</th>\n",
       "      <th>tempo</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acurácia de KNeighbors simples:</td>\n",
       "      <td>0.976715</td>\n",
       "      <td>0.162706</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Acurácia de KNeighbors Bagging (c/ 100 estimat...</td>\n",
       "      <td>0.973409</td>\n",
       "      <td>2.291666</td>\n",
       "      <td>redução</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Acurácia de Random Forest:</td>\n",
       "      <td>0.973396</td>\n",
       "      <td>0.537558</td>\n",
       "      <td>redução</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Acurácia de Random Forest:</td>\n",
       "      <td>0.971753</td>\n",
       "      <td>0.482972</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acurácia de KNeighbors Bagging (c/ 100 estimat...</td>\n",
       "      <td>0.971746</td>\n",
       "      <td>3.823201</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acurácia de KNeighbors Bagging (c/ 10 estimato...</td>\n",
       "      <td>0.970916</td>\n",
       "      <td>0.368454</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Acurácia de KNeighbors simples:</td>\n",
       "      <td>0.969239</td>\n",
       "      <td>0.103337</td>\n",
       "      <td>redução</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Acurácia de KNeighbors Bagging (c/ 10 estimato...</td>\n",
       "      <td>0.962593</td>\n",
       "      <td>0.253812</td>\n",
       "      <td>redução</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acurácia de Random Forest:</td>\n",
       "      <td>0.959284</td>\n",
       "      <td>0.641312</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acurácia de Gradient Boosting Tree:</td>\n",
       "      <td>0.957611</td>\n",
       "      <td>11.849338</td>\n",
       "      <td>originais</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Modelo  Acuracia      tempo  \\\n",
       "0                     Acurácia de KNeighbors simples:  0.976715   0.162706   \n",
       "9   Acurácia de KNeighbors Bagging (c/ 100 estimat...  0.973409   2.291666   \n",
       "12                         Acurácia de Random Forest:  0.973396   0.537558   \n",
       "5                          Acurácia de Random Forest:  0.971753   0.482972   \n",
       "2   Acurácia de KNeighbors Bagging (c/ 100 estimat...  0.971746   3.823201   \n",
       "1   Acurácia de KNeighbors Bagging (c/ 10 estimato...  0.970916   0.368454   \n",
       "7                     Acurácia de KNeighbors simples:  0.969239   0.103337   \n",
       "8   Acurácia de KNeighbors Bagging (c/ 10 estimato...  0.962593   0.253812   \n",
       "4                          Acurácia de Random Forest:  0.959284   0.641312   \n",
       "6                 Acurácia de Gradient Boosting Tree:  0.957611  11.849338   \n",
       "\n",
       "         tipo  \n",
       "0   originais  \n",
       "9     redução  \n",
       "12    redução  \n",
       "5   originais  \n",
       "2   originais  \n",
       "1   originais  \n",
       "7     redução  \n",
       "8     redução  \n",
       "4   originais  \n",
       "6   originais  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analise.sort_values(by=['Acuracia'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O modelo que teve a melhor acuracia com relacao ao tempo foi KNeighbors simples com os dados originais, sem tranformação ou seleção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acurácia de KNeighbors simples: 0.9767150760719225 \n",
    "Tempo: 0.24569296836853027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Para melhor visualização dos dados, anexamos o report em arquivo .docx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
